{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db78cf9c",
   "metadata": {},
   "source": [
    "# EEG Example (Parcel Level)\n",
    "\n",
    "Start-to-end example script for applying the canonical HMM to EEG data. Note, because we source reconstruct, we need medium/high-density EEG: ~64+ channels.\n",
    "\n",
    "In this example, we will use a standard brain in the source reconstruction, so only the EEG data is required.\n",
    "\n",
    "#### Input Data\n",
    "\n",
    "We will use data in BIDS format:\n",
    "```\n",
    "BIDS/\n",
    "├── ...\n",
    "├── sub-04/\n",
    "│   ├── eeg/\n",
    "│   │   ├── 01_rest_eyecolsed_raw.fif\n",
    "│   │   ├── ...\n",
    "├── ...\n",
    "```\n",
    "And write output to the `BIDS/derivatives` directory.\n",
    "\n",
    "#### Steps\n",
    "\n",
    "1. Preprocessing:\n",
    "    - We clean the sensor-level EEG data and save the output to `BIDS/derivatives/preprocessed`.\n",
    "\n",
    "2. Surface Extraction:\n",
    "    - We use surfaces from a standard brain (MNI152) extracted with FSL.\n",
    "\n",
    "3. Coregistration:\n",
    "    - We will coregister the EEG electrodes with the structural MRI using Polhemus headshape points/fiducials and the surfaces from the MRI.\n",
    "    - We will also compute the forward model.\n",
    "\n",
    "4. Source Reconstruction (LCMV Beamformer):\n",
    "    - We will compute a volumetric LCMV beamformer and apply it to the cleaned sensor data to get data in a voxel grid.\n",
    "\n",
    "5. Parcellation:\n",
    "    - We will parcel the data and orthogonalise to remove spatial leakage.\n",
    "    - The output of steps 2-5 will be saved to `BIDS/derivatives/osl`.\n",
    "\n",
    "6. Prepare Data for the Canonical HMM:\n",
    "    - We will perform dipole sign flipping, time-delay embedding, PCA and standardisation to prepare the parcel data for the canonical HMM.\n",
    "\n",
    "6. Prepare Data for the Canonical HMM:\n",
    "    - We will perform dipole sign flipping, time-delay embedding, PCA and standardisation to prepare the parcel data for the canonical HMM.\n",
    "\n",
    "7. Fit the Canonical HMM:\n",
    "    - We will apply the canonical HMM to the prepared data.\n",
    "\n",
    "8. HMM Post-Hoc Analysis:\n",
    "    - Estimate state-specific quantities of interest, such as spectral properties, networks, and summary statistics for dynamics.\n",
    "    - The output will be saved to `BIDS/derivatives/hmm`.\n",
    "\n",
    "After Step 5, if your data includes multiple subjects ($N > 2$), Sign Flipping needs to be performed to eliminate the sign ambiguity introduced by source reconstruction and PCA. This process selects a template subject based on the TDE covariance matrices and uses a random search algorithm to align the signs of all other subjects' time series to that template (Note: this tutorial only uses a single subject, so this code is omitted).\n",
    "\n",
    " Although we provide a single start-to-end example script, it is possible to separate the different steps. E.g. you may wish to do all the preprocessing (for different sessions) in a script then have a separate script for the coregistration, source reconstruction and parcellation, as well as a different script for the HMM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77007b0f",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45dbba1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:21:05.420975: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using notebook 3d backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "from pathlib import Path\n",
    "from modules import preproc, rhino, source_recon, parcellation, hmm, utils\n",
    "from osl_dynamics import inference, analysis\n",
    "from osl_dynamics.utils import plotting\n",
    "from osl_dynamics.data import Data\n",
    "import mne\n",
    "mne.viz.set_3d_backend(\"notebook\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08933515",
   "metadata": {},
   "source": [
    "#### Session Info\n",
    "\n",
    "Let's specify which session in the `BIDS` directory we wish to study.\n",
    "\n",
    "We will use the session `01_rest_eyecolsed_raw.fif` for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb4c04a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = \"01_rest_eyecolsed\"\n",
    "session_id = \"sub-01\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec02d9",
   "metadata": {},
   "source": [
    "For EEG data, we need to add the sensor location information to the mne Raw object (needed for source reconstruction). There's no standardised way people save this information and you will likely need to write your own custom function specific to your dataset to add this information.\n",
    "\n",
    "Note, if you do not have sensor locations, you could use a [standard montage](https://mne.tools/stable/auto_tutorials/intro/40_sensor_locations.html#sphx-glr-auto-tutorials-intro-40-sensor-locations-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6ab3b7",
   "metadata": {},
   "source": [
    "## 1. Preprocessing\n",
    "\n",
    "Now, we can clean the sensor data. We will use MNE to do this. Let's load the raw fif file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ccf2986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file BIDS/sub-01/eeg/01_rest_eyecolsed_raw.fif...\n",
      "    Range : 2000 ... 355480 =      2.000 ...   355.480 secs\n",
      "Ready.\n",
      "Reading 0 ... 353480  =      0.000 ...   353.480 secs...\n"
     ]
    }
   ],
   "source": [
    "raw = mne.io.read_raw_fif(f\"BIDS/{session_id}/eeg/{subject_id}_raw.fif\", preload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6622fd5",
   "metadata": {},
   "source": [
    "Now, let's perform some minimal preprocessing: filtering, downsampling and bad segment detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "633d1936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 1e+02 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 20 (effective, after forward-backward)\n",
      "- Cutoffs at 1.00, 100.00 Hz: -6.02, -6.02 dB\n",
      "\n",
      "\n",
      "Bad segment detection\n",
      "---------------------\n",
      "Modality: eeg\n",
      "Mode: None\n",
      "Metric: std\n",
      "Significance level: 0.1\n",
      "Maximum fraction: 0.1\n",
      "Found 0 bad segments: 0.0/353.5 seconds rejected (0.0%)\n",
      "\n",
      "Bad segment detection\n",
      "---------------------\n",
      "Modality: eeg\n",
      "Mode: diff\n",
      "Metric: std\n",
      "Significance level: 0.1\n",
      "Maximum fraction: 0.1\n",
      "Found 0 bad segments: 0.0/353.5 seconds rejected (0.0%)\n",
      "\n",
      "Bad segment detection\n",
      "---------------------\n",
      "Modality: eeg\n",
      "Mode: None\n",
      "Metric: kurtosis\n",
      "Significance level: 0.1\n",
      "Maximum fraction: 0.1\n",
      "Found 2 bad segments: 4.0/353.5 seconds rejected (1.1%)\n"
     ]
    }
   ],
   "source": [
    "raw = raw.filter(l_freq=1, h_freq=100, method=\"iir\", iir_params={\"order\": 5, \"ftype\": \"butter\"})\n",
    "raw = raw.resample(sfreq=250)\n",
    "raw = preproc.detect_bad_segments(raw, picks=\"eeg\", significance_level=0.1)\n",
    "raw = preproc.detect_bad_segments(raw, picks=\"eeg\", mode=\"diff\", significance_level=0.1)\n",
    "raw = preproc.detect_bad_segments(raw, picks=\"eeg\", metric=\"kurtosis\", significance_level=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e98f7",
   "metadata": {},
   "source": [
    "Then, we use [ICLabel](https://mne.tools/mne-icalabel/stable/index.html) to identify abnormal components in the EEG data, classifying components with a high confidence score as abnormal and subsequently removing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849fba71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ICA Automatic Artifact Removal (Method: iclabel)\n",
      "----------------------------\n",
      "Filtering data copy (1-100Hz) for ICA fitting...\n",
      "Applying average reference (ICLabel requirement)...\n",
      "Fitting ICA (method='infomax', n_components=30, picks=eeg)...\n",
      "Fitting ICA to data using 62 channels (please be patient, this may take a while)\n",
      "Omitting 1000 of 88370 (1.13%) samples, retaining 87370 (98.87%) samples.\n",
      "Selecting by number: 30 components\n",
      "Computing Extended Infomax ICA\n"
     ]
    }
   ],
   "source": [
    "raw = preproc.ica_ICLabel(raw, picks=\"eeg\",n_components=30,threshold=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9441666a",
   "metadata": {},
   "source": [
    "Finally, we apply an average re-referencing (needed for source reconstruction) and we save the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b232ef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw = raw.drop_channels([\"VEOG\"]) # if you have VEOG channel\n",
    "raw = raw.set_eeg_reference(projection=True)\n",
    "\n",
    "preproc_file = Path(f\"BIDS/derivatives/preprocessed/{session_id}/{subject_id}_preproc-raw.fif\")\n",
    "preproc_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "raw.save(preproc_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beed4be",
   "metadata": {},
   "source": [
    "## 2. Surface Extraction\n",
    "\n",
    "We need to extract surfaces (inskull, outskull and outskin) from a structural MRI before we can perform the coregistration. In this example we will use a standard brain. However, if you do have a structural you can extract the surfaces with:\n",
    "```\n",
    "rhino.extract_surfaces(\n",
    "    smri_file=smri_file,\n",
    "    outdir=f\"BIDS/derivatives/anat_surfaces/sub-{subject}\",\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ac5d0",
   "metadata": {},
   "source": [
    "## OSL Output Files\n",
    "\n",
    "Now we have the preprocessed sensor data and the MRI surfaces. We can perform an 'OSL' source reconstruction. This involves creating many files. Let's create an object (container) to keep track of all the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b104dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = utils.OSLFilenames(\n",
    "    outdir=f\"BIDS/derivatives/osl/{session_id}\",\n",
    "    id=subject_id,\n",
    "    preproc_file=f\"BIDS/derivatives/preprocessed/{session_id}/{subject_id}_preproc-raw.fif\",\n",
    "    surfaces_dir=\"mni152_surfaces\",  # replace with the 'outdir' used in rhino.extract_surfaces if you have your own structural\n",
    ")\n",
    "print(fns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fa35ac",
   "metadata": {},
   "source": [
    "You can adjust the headshape, nasion, LPA, and RPA coordinates here to match head model and fiducial files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed13f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_headshape_fiducials(coreg_dir):\n",
    "    \"\"\"\n",
    "    Directly modifies the headshape and fiducial files in the specified coreg directory.\n",
    "    \n",
    "    Parameters:\n",
    "    coreg_dir: str, The path to the folder containing the Polhemus files.\n",
    "               Example: 'eeg_data/osl/01_rest_eyecolsed/coreg'\n",
    "    \"\"\"\n",
    "    # 1. Manually construct file paths (assuming default OSL generated filenames)\n",
    "    # Based on the logs in eeg.ipynb, the standard filenames are as follows:\n",
    "    filenames = {\n",
    "        \"polhemus_headshape_file\": os.path.join(coreg_dir, \"polhemus_headshape.txt\"),\n",
    "        \"polhemus_nasion_file\":    os.path.join(coreg_dir, \"polhemus_nasion.txt\"),\n",
    "        \"polhemus_lpa_file\":       os.path.join(coreg_dir, \"polhemus_lpa.txt\"),\n",
    "        \"polhemus_rpa_file\":       os.path.join(coreg_dir, \"polhemus_rpa.txt\"),\n",
    "    }\n",
    "\n",
    "    # Check if files exist to avoid errors\n",
    "    for key, fpath in filenames.items():\n",
    "        if not os.path.exists(fpath):\n",
    "            raise FileNotFoundError(f\"File not found: {fpath}. Please check the path.\")\n",
    "\n",
    "    # 2. Load the saved headshape and fiducial files\n",
    "    hs = np.loadtxt(filenames[\"polhemus_headshape_file\"])\n",
    "    nas = np.loadtxt(filenames[\"polhemus_nasion_file\"])\n",
    "    lpa = np.loadtxt(filenames[\"polhemus_lpa_file\"])\n",
    "    rpa = np.loadtxt(filenames[\"polhemus_rpa_file\"])\n",
    "    \n",
    "    # 3. Modify coordinates (Move fiducials)\n",
    "    # Scale the headshape\n",
    "    hs *= 1.3\n",
    "    # Move along Z-axis (units: mm)\n",
    "    nas[2] -= 70\n",
    "    lpa[2] -= 60\n",
    "    rpa[2] -= 60\n",
    "    # Move along Y-axis (units: mm)\n",
    "    nas[1] += 5\n",
    "    lpa[1] -= 20\n",
    "    rpa[1] -= 20\n",
    "    # Move along X-axis (units: mm)\n",
    "    nas[0] += 0 \n",
    "    lpa[0] += 0\n",
    "    rpa[0] -= 0\n",
    "    \n",
    "    # 4. Overwrite and save files\n",
    "    print(f\"Updating files in: {coreg_dir}\")\n",
    "    np.savetxt(filenames[\"polhemus_nasion_file\"], nas)\n",
    "    np.savetxt(filenames[\"polhemus_lpa_file\"], lpa)\n",
    "    np.savetxt(filenames[\"polhemus_rpa_file\"], rpa)\n",
    "    np.savetxt(filenames[\"polhemus_headshape_file\"], hs)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9bb682",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_coreg_path = f\"BIDS/derivatives/osl/{session_id}/{subject_id}/coreg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca14e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhino.extract_polhemus_from_fif(fns, include_eeg_as_headshape=True)\n",
    "fix_headshape_fiducials(target_coreg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4566a3",
   "metadata": {},
   "source": [
    "There are some files which we have already generated like the preprocessed fif file and the surfaces. Other files we will create in the following steps.\n",
    "\n",
    "Now let's extract the Polhemus from the preprocessed fif file and coregister the MEG and MRI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f56c947",
   "metadata": {},
   "source": [
    "## 3. Coregistration\n",
    "\n",
    "Next we use the Polhemus headshape points/fiducials to coregister the MEG and structural MRI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d4a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhino.coregister(\n",
    "    fns,\n",
    "    allow_smri_scaling=True,  # set to False if using a real structural\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081e97b6",
   "metadata": {},
   "source": [
    "Here, because we're using a standard brain, the EEG locations don't perfectly match the scalp. However, because we have real sensor locations, we'll leave them where they are.\n",
    "\n",
    "Next, we calculate the forward model for MEG. Here, we're using a 'Triple Layer' head model and a dipole grid resolution of 8mm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100e75ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhino.forward_model(fns, model=\"Triple Layer\", gridstep=8, meg=False, eeg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ab6c1",
   "metadata": {},
   "source": [
    "## 4. Source Reconstruction\n",
    "\n",
    "Now we are ready to create an LCMV beamformer. We will use a unit-noise-gain-invariant beamformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8c8d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_recon.lcmv_beamformer(fns, raw, chantypes=\"eeg\", rank={\"eeg\": 50})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ade653",
   "metadata": {},
   "source": [
    "Next, we calculate voxel data by applying the beamformer weights to the sensor data. Note, the following function returns the voxel data  (and coordinates) in MNI space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f221236",
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_data, voxel_coords = source_recon.apply_lcmv_beamformer(fns, raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1cb2f",
   "metadata": {},
   "source": [
    "## 5. Parcellation\n",
    "\n",
    "Now we have the voxel data, we can parcellate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "parcellation_file = \"fmri_d100_parcellation_with_PCC_reduced_2mm_ss5mm_ds8mm.nii.gz\"\n",
    "\n",
    "parcel_data = parcellation.parcellate(\n",
    "    fns,\n",
    "    voxel_data,\n",
    "    voxel_coords,\n",
    "    method=\"spatial_basis\",\n",
    "    orthogonalisation=\"symmetric\",\n",
    "    parcellation_file=parcellation_file,\n",
    ")\n",
    "\n",
    "parc_fif = f\"BIDS/derivatives/osl/{session_id}/{subject_id}/lcmv-parc-raw.fif\"\n",
    "parcellation.save_as_fif(\n",
    "    parcel_data,\n",
    "    raw,\n",
    "    extra_chans=\"stim\",\n",
    "    filename=parc_fif,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba336ef9",
   "metadata": {},
   "source": [
    "The final data is saved in the file `lcmv-parc-raw.fif`. The parcellated data is saves as a `'misc'` channel type.\n",
    "\n",
    "The last thing we will do is plot the PSD of the parcel data to check the source reconstruction looks reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d31256",
   "metadata": {},
   "outputs": [],
   "source": [
    "parcellation.plot_psds(parc_fif, parcellation_file=parcellation_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c45063",
   "metadata": {},
   "source": [
    "We should see alpha (~10 Hz) activity is posterior in the brain if not there may be an issue. Although note, there is a lot of subject variability, some subjects don't exhibit strong alpha oscillations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79404aa",
   "metadata": {},
   "source": [
    "## 6. Prepare Data for the Canonical HMM\n",
    "\n",
    "Here, we prepare the data by aligning the sign of the parcel time courses to the template session from the canonical HMM and then perform time-delay embedding and PCA. Finally, we standardise the data. This is all done with the `prepare_data_for_canonical_hmm` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the data\n",
    "data = Data(parc_fif, picks=\"misc\", reject_by_annotation=\"omit\")\n",
    "data = hmm.prepare_data_for_canonical_hmm(data, parcellation=\"38ROI_Giles\")\n",
    "\n",
    "# Save\n",
    "hmm_dir = f\"BIDS/derivatives/hmm/{session_id}/{subject_id}\"\n",
    "data.save(hmm_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacd3573",
   "metadata": {},
   "source": [
    "## 7. Fit the Canonical HMM\n",
    "\n",
    "First, we load a canonical HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a14fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hmm.load_canonical_hmm(n_states=8, parcellation=\"38ROI_Giles\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb3bef",
   "metadata": {},
   "source": [
    "Now let's infer the probability of each state being active at a given time point in the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af2d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State probability time course\n",
    "alp = model.get_alpha(data)\n",
    "\n",
    "# Save\n",
    "alp_raw = inference.modes.convert_to_mne_raw(alp, parc_fif, n_embeddings=data.n_embeddings)\n",
    "alp_raw.save(f\"{hmm_dir}/alp_raw.fif\", overwrite=True)\n",
    "\n",
    "# Plot (just the first 8 seconds for visualisation)\n",
    "fig, ax = plotting.plot_alpha(alp, n_samples=1500, sampling_frequency=alp_raw.info[\"sfreq\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4434e10",
   "metadata": {},
   "source": [
    "## 8. HMM Post-Hoc Analysis\n",
    "\n",
    "Now we have the state probabilities, we can perform 'post-hoc analysis' where we estimate state-specific quantities of interest such as spectral properties, networks, and summary statistics for dynamics.\n",
    "\n",
    "See the [osl-dynamics docs](https://osl-dynamics.readthedocs.io/en/latest/documentation.html) for further information and tutorials, as well as code for visualisations.\n",
    "\n",
    "Let's start with the spectral properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a1ceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate multitaper\n",
    "trimmed_data = data.trim_time_series(sequence_length=model.config.sequence_length, prepared=False)\n",
    "f, psd, coh = analysis.spectral.multitaper_spectra(\n",
    "    data=trimmed_data,\n",
    "    alpha=alp,\n",
    "    sampling_frequency=250,\n",
    "    frequency_range=[0.5, 45],\n",
    ")\n",
    "print(f.shape)\n",
    "print(psd.shape)\n",
    "print(coh.shape)\n",
    "\n",
    "# Save\n",
    "np.save(f\"{hmm_dir}/f.npy\", f)\n",
    "np.save(f\"{hmm_dir}/psd.npy\", psd)\n",
    "np.save(f\"{hmm_dir}/coh.npy\", coh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81dea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power maps\n",
    "pow_maps = analysis.power.variance_from_spectra(f, psd)\n",
    "print(pow_maps.shape)\n",
    "np.save(f\"{hmm_dir}/pow_maps.npy\", pow_maps)\n",
    "\n",
    "# Coherence networks\n",
    "coh_nets = analysis.connectivity.mean_coherence_from_spectra(f, coh)\n",
    "print(coh_nets.shape)\n",
    "np.save(f\"{hmm_dir}/coh_nets.npy\", coh_nets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d1df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarise the state probabilities\n",
    "stc = inference.modes.argmax_time_courses(alp)\n",
    "\n",
    "# Summary statistics\n",
    "fo = analysis.post_hoc.fractional_occupancies(stc)\n",
    "lt = analysis.post_hoc.mean_lifetimes(stc, sampling_frequency=raw.info[\"sfreq\"])\n",
    "intv = analysis.post_hoc.mean_intervals(stc, sampling_frequency=raw.info[\"sfreq\"])\n",
    "sr = analysis.post_hoc.switching_rates(stc, sampling_frequency=raw.info[\"sfreq\"])\n",
    "\n",
    "# Save\n",
    "np.save(f\"{hmm_dir}/fo.npy\", fo)\n",
    "np.save(f\"{hmm_dir}/lt.npy\", lt)\n",
    "np.save(f\"{hmm_dir}/intv.npy\", intv)\n",
    "np.save(f\"{hmm_dir}/sr.npy\", sr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osld",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
